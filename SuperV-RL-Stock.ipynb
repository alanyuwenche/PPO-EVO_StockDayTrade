{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"KXjzTiJM3UZC","colab":{"base_uri":"https://localhost:8080/","height":75},"executionInfo":{"status":"ok","timestamp":1674868482298,"user_tz":-480,"elapsed":153184,"user":{"displayName":"Yu Alan","userId":"07954502632401534355"}},"outputId":"c9e4443c-dc0b-45ee-cda2-0878907da392"},"source":["from google.colab import files\n","uploaded = files.upload()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-d4b5c0ba-f3fd-479d-b837-f9f33e847cfa\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-d4b5c0ba-f3fd-479d-b837-f9f33e847cfa\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving train_N.pickle to train_N.pickle\n"]}]},{"cell_type":"code","source":["ls"],"metadata":{"id":"RhKpFud6c8ju","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1674782126004,"user_tz":-480,"elapsed":482,"user":{"displayName":"Yu Alan","userId":"07954502632401534355"}},"outputId":"40e5c03f-289f-4696-98c5-98822f4a07fc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epi-16256.pth  \u001b[0m\u001b[01;34msample_data\u001b[0m/\n"]}]},{"cell_type":"code","source":["!pip install tensorboardX"],"metadata":{"id":"hCvW30en2hdF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#%load_ext tensorboard\n","%reload_ext tensorboard\n","%tensorboard --logdir=/content/runs/20230116"],"metadata":{"id":"3Ex9VHqf2bI9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#!tar -cvf data.tar data/\n","!gzip data.tar\n","#!tar zxvf data.tar.gz #解壓縮(須先改名)\n","#!unzip data.zip"],"metadata":{"id":"8nHp7IgrJzvB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##################   My   Code   ###########################"],"metadata":{"id":"Y1TISyYZELck"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import pickle\n","\n","from collections import Counter\n","from tensorboardX import SummaryWriter\n","\n","with open('train_N.pickle','rb') as f:\n","    data = pickle.load(file = f)#data=> list\n","dataN = len(data)\n","\n","df = pd.DataFrame(data, columns =['Price', 'Symb'])\n","df.insert(2, \"dailyR\", np.nan)\n","for i in range(1,dataN):\n","  if df['Symb'][i][3:5] == df['Symb'][i-1][3:5]:\n","    df.at[i, 'dailyR'] = np.log(df['Price'][i].iloc[283,1])-np.log(df['Price'][i-1].iloc[283,1])\n","\n","clP = np.zeros(dataN)\n","for j in range(dataN):\n","  clP[j] = df['Price'][j].iloc[283,1]#每日13:29的價\n","pM = clP.mean()\n","pS = clP.std()\n","clPNorm =  (clP - pM) / pS\n","\n","class Reporter:\n","    def __init__(self, report_interval: int = 1):\n","        self.counter = Counter()\n","        self.graph_initialized = False\n","        self.report_interval = report_interval\n","        self.t = 0\n","        self.writer = SummaryWriter('runs/20230116')#\n","\n","    def will_report(self, tag: str) -> bool:\n","        return self.counter[tag] % (self.report_interval + 1) == 0\n","\n","    def scalar(self, tag: str, value: float):\n","        if self.will_report(tag):\n","            self._scalar(tag, value, self.counter[tag])\n","        self.counter[tag] += 1\n","\n","    def graph(self, model, input_to_model):\n","        if not self.graph_initialized:\n","            self._graph(model, input_to_model)\n","            self.graph_initialized = True\n","\n","    def _scalar(self, tag: str, value: float, step: int):\n","        self.writer.add_scalar(tag, value, step)\n","\n","class observation_space:\n","    def __init__(self, n):\n","        self.shape = (n,)\n","\n","class action_space:\n","    def __init__(self, n):\n","        self.n = n\n","    def seed(self, seed):\n","        pass\n","    def sample(self):\n","        return random.randint(0, self.n - 1)\n","\n","class Finance:\n","    def __init__(self, features, window, lags):\n","        self.features = features\n","        self.observation_space = observation_space(len(features))\n","        self.action_space = action_space(3)#ori:2  ????????????????? \n","        self.window = window#如5日均值\n","        self.lags = lags #相當於batach_size\n","        self.osn = self.lags       \n","        self.min_accuracy = 0.475\n","        self._prepare_data()\n","\n","    def _prepare_data(self):\n","        self.data = df\n","        self.data['r'] = df['dailyR']\n","        self.data.dropna(inplace=True)       \n","        #self.data['d'] = np.where(self.data['r'] > 0, 1, 0) #ori\n","        self.data['d'] = np.where(self.data['r'] > 0, 1, 2)\n","        self.data['clPNorm'] = pd.DataFrame(clPNorm)\n","        self.data['rNorm'] = (self.data['r']-self.data['r'].mean())/self.data['r'].std()\n","        self.data['clPNormAvg'] = self.data['clPNorm'].rolling(self.window).mean()\n","        self.data['clPNormStd'] = self.data['clPNorm'].rolling(self.window).std()\n","        self.data['rNormAvg'] = self.data['rNorm'].rolling(self.window).mean()\n","        self.data['rNormStd'] = self.data['rNorm'].rolling(self.window).std()\n","        self.data.dropna(inplace=True)#這行會使平均值,標準差的NaN都消失!!!!!!!!!!!!!!!!\n","\n","    def _get_state(self):\n","        return self.data[self.features].iloc[self.bar-self.lags : self.bar]\n","\n","    def reset(self):\n","        self.treward = 0\n","        self.accuracy = 0\n","        self.bar = self.osn#必須先要reset才會有self.bar, 使用step(a)才不會出錯\n","        state = self.data[self.features].iloc[self.bar-self.osn : self.bar]#會按照順序\n","        return state.values\n","\n","    def step(self, action):\n","        correct = action == self.data['d'].iloc[self.bar]\n","        #ret = self.data['rNorm'].iloc[self.bar]\n","        reward_1 = 1 if correct else 0\n","        self.treward += reward_1\n","        self.bar += 1\n","        self.accuracy = self.treward / (self.bar - self.osn)\n","\n","        if self.bar >= len(learn_env.data):\n","            done = True\n","        elif reward_1 == 1:\n","            done = False\n","        elif ((self.accuracy < self.min_accuracy) and (self.bar > self.osn + 10)):# Xa.ipynb\n","            done = True\n","        else:\n","            done = False\n","      \n","        state = self._get_state()\n","        info = {}\n","\n","        return state.values, reward_1, done, info\n","\n","#features = ['clPNorm','rNorm']\n","features = ['clPNorm','rNorm','clPNormAvg','clPNormStd','rNormAvg','rNormStd']\n","window = 5 #平均值時會用到\n","lags = 4\n","learn_env = Finance(features, window, lags)#\n","\n","\"\"\"\n","(self, features, window, lags)\n","learn_env.reset()\n","#s = learn_env.reset()\n","\n","clP: 590個日收盤價\n","learn_env.data['rNorm'].iloc[0] => nan\n","learn_env.data.head(10)\n","learn_env.data['dailyR'][14:22]\n","\"\"\""],"metadata":{"id":"GsG_5h2s8G2d","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1674868538724,"user_tz":-480,"elapsed":7404,"user":{"displayName":"Yu Alan","userId":"07954502632401534355"}},"outputId":"c65c9d84-c5e6-4934-eeb8-29bb32efac85"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\n(self, features, window, lags)\\nlearn_env.reset()\\n#s = learn_env.reset()\\n\\nclP: 590個日收盤價\\nlearn_env.data['rNorm'].iloc[0] => nan\\nlearn_env.data.head(10)\\nlearn_env.data['dailyR'][14:22]\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["import random\n","import torch\n","import torch.nn as nn\n","from torch.distributions import Categorical\n","from torch.optim import Adam\n","import torch.nn.functional as F\n","from collections import deque\n","import os\n","\n","class CategoricalPolicy(nn.Module):\n","    def __init__(self, hidden_size, state_dim=6, action_dim=3):#\n","        super(CategoricalPolicy, self).__init__()\n","        self.state_dim = state_dim\n","        self.action_dim = action_dim\n","\n","        ######################## Q1 Head ##################\n","        # Construct Hidden Layer 1 with state\n","        self.f1 = nn.Linear(state_dim, hidden_size)\n","\n","        #Hidden Layer 2\n","        self.f2 = nn.Linear(hidden_size, hidden_size)\n","\n","        #Value\n","        self.val = nn.Linear(hidden_size, 1)\n","\n","        #Advantages\n","        self.adv = nn.Linear(hidden_size, action_dim)\n","\n","\n","\n","    def clean_action(self, obs, return_only_action=True):\n","        ###### Feature ####\n","        info = torch.relu(self.f1(obs))#RuntimeError: mat1 and mat2 must have the same dtype\n","        info = torch.relu(self.f2(info))\n","\n","        val = self.val(info)\n","        adv = self.adv(info)\n","\n","        logits = val + adv - adv.mean()\n","\n","        if return_only_action:\n","            return logits.argmax(1)\n","\n","        return None, None, logits\n","\n","    def noisy_action(self, obs, return_only_action=True):\n","        _, _, logits = self.clean_action(obs, return_only_action=False)\n","        dist = Categorical(logits=logits)\n","        action = dist.sample()\n","        action = action\n","\n","        if return_only_action:\n","            return action\n","\n","        return action, None, logits\n","\n","def hard_update(target, source):\n","    for target_param, param in zip(target.parameters(), source.parameters()):\n","        target_param.data.copy_(param.data)\n","\n","def soft_update(target, source, tau):\n","    for target_param, param in zip(target.parameters(), source.parameters()):\n","        target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)\n","\n","class FQLAgent:\n","    def __init__(self, hidden_dim, learn_env, learning_rate=0.0001, alpha=0.1, tau=1e-3):\n","        self.learn_env = learn_env\n","        self.epsilon = 1.0\n","        self.epsilon_min = 0.1\n","        self.epsilon_decay = 0.98\n","        self.learning_rate = learning_rate\n","        self.gamma = 0.95\n","        self.alpha = alpha #Alpha for Entropy term\n","        self.tau = tau #soft_update param. \n","        self.batch_size = 128\n","        self.max_treward = 0\n","        self.trewards = list()\n","        self.averages = list()\n","        self.performances = list()\n","        self.memory = deque(maxlen=2000)\n","        self.model = CategoricalPolicy(hidden_dim)\n","        self.model_target = CategoricalPolicy(hidden_dim)\n","        hard_update(self.model_target, self.model)\n","        self.model_optim = Adam(self.model.parameters(), lr=self.learning_rate)\n","        self.reporter = Reporter()\n","\n","    def act(self, state):\n","        state = torch.tensor(state, dtype=torch.float32)\n","        \"\"\"      \n","        if random.random() <= self.epsilon:\n","            return self.learn_env.action_space.sample()\n","        \"\"\"            \n","        #action, _, _ = self.model.noisy_action(state, return_only_action=False)\n","        action = self.model.noisy_action(state, return_only_action=True)\n","\n","        #return torch.argmax(action)\n","        return action[0]\n","\n","\n","    def replay(self):\n","        batch = random.sample(self.memory, self.batch_size)\n"," \n","        for state, action, reward, next_state, done in batch:\n","          state = torch.tensor(state, dtype=torch.float32)\n","          next_state = torch.tensor(next_state, dtype=torch.float32)\n","          action = action * torch.ones((1,self.model.action_dim), dtype=torch.int64)#My Code-將action從單一數字改維度,才能作gather計算\n","          d = 1 if done else 0\n","\n","          with torch.no_grad():\n","            na = self.model.clean_action(next_state,  return_only_action=True)\n","            _, _, ns_logits = self.model_target.noisy_action(next_state, return_only_action=False)\n","            next_entropy = -(F.softmax(ns_logits, dim=1) * F.log_softmax(ns_logits, dim=1)).mean(1).unsqueeze(1)\n","            ns_logits = ns_logits.gather(1, na.unsqueeze(1))#20221114 取出action相應的對數機率\n","            next_target = ns_logits + self.alpha * next_entropy\n","            next_q_value = reward + (1-d) * self.gamma * next_target\n","\n","        _, _, logits  = self.model.noisy_action(state, return_only_action=False)\n","        entropy = -(F.softmax(logits, dim=1) * F.log_softmax(logits, dim=1)).mean(1).unsqueeze(1)\n","        q_val = logits.gather(1, action)\n","\n","        q_loss = (next_q_value - q_val)**2\n","        q_loss -= self.alpha*entropy\n","        q_loss = q_loss.mean()\n","\n","        self.model_optim.zero_grad()\n","        q_loss.backward()\n","        torch.nn.utils.clip_grad_norm_(parameters=self.model.parameters(), max_norm=10)#\n","        self.model_optim.step()\n","\n","        soft_update(self.model_target, self.model, self.tau)\n","\n","        self.reporter.scalar('q_loss/total', q_loss)\n","\n","    def learn(self, episodes):\n","        os.makedirs('./data/', exist_ok=True)\n","        for e in range(1, episodes + 1):\n","            state = self.learn_env.reset()\n","            rewardT = 0\n","            for _ in range(10000):\n","                action = self.act(state)\n","                next_state, reward, done, info = self.learn_env.step(action)\n","                rewardT += reward\n","                self.memory.append([state, action, reward, next_state, done])\n","              \n","                state = next_state\n","                if done:\n","                    treward = _ + 1\n","                    self.trewards.append(treward)\n","                    av = sum(self.trewards[-len(self.trewards):]) / len(self.trewards)\n","                    perf = self.learn_env.accuracy\n","                    self.averages.append(av)\n","                    self.performances.append(perf)\n","                    self.max_treward = max(self.max_treward, treward)\n","                    templ = 'episode: {:2d}/{} | treward: {:4d} | '\n","                    templ += 'perf: {:5.3f} | av: {:5.1f} | max: {:4d}'\n","                    print(templ.format(e, episodes, treward, perf, av, self.max_treward))\n","                    #if treward > 500:\n","                    if rewardT > 270:\n","                      f = open(\"./data/logfile.txt\",\"a\")\n","                      f.write('Eposode: %d\\t' % e)\n","                      f.write(\"treward: %d\\t\" % treward)\n","                      f.write(\"perf: %d\\t\" % perf)\n","                      f.write(\"av: %d\\t\" % av)\n","                      f.write(\"rewardT: %d\\t\" % rewardT)\n","                      f.write(\"max: %d\\t\" % self.max_treward)\n","                      f.write(\"\\n\")\n","                      f.close()\n","                      fileN = './data/Epi-'+str(e)+'.pth'\n","                      torch.save(self.model.state_dict(),fileN)\n","\n","                    break\n","            if len(self.memory) > self.batch_size:\n","                self.replay()\n","\"\"\"\n","for i in range(1,dataN):\n","  if data[i][1][3:5] == data[i-1][1][3:5]:\n","    ret = np.log(data[i][0].iloc[283,1])-np.log(data[i-1][0].iloc[283,1])\n","    print('i: ',i,' ',ret)\n","\"\"\""],"metadata":{"id":"0qSQcB10QiPM","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1674868597692,"user_tz":-480,"elapsed":868,"user":{"displayName":"Yu Alan","userId":"07954502632401534355"}},"outputId":"0c817c0f-33a4-4641-d655-fffe52b49abb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\nfor i in range(1,dataN):\\n  if data[i][1][3:5] == data[i-1][1][3:5]:\\n    ret = np.log(data[i][0].iloc[283,1])-np.log(data[i-1][0].iloc[283,1])\\n    print('i: ',i,' ',ret)\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["agent = FQLAgent(256, learn_env)"],"metadata":{"id":"ca_jqvgbqlRM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%time agent.learn(5)"],"metadata":{"id":"ccXc9Nrcf8Dk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1674868636924,"user_tz":-480,"elapsed":271,"user":{"displayName":"Yu Alan","userId":"07954502632401534355"}},"outputId":"d70756cc-1956-44b1-bd96-96f8ae2c00ef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["episode:  1/5 | treward:   11 | perf: 0.182 | av:  11.0 | max:   11\n","episode:  2/5 | treward:   12 | perf: 0.417 | av:  11.5 | max:   12\n","episode:  3/5 | treward:   12 | perf: 0.250 | av:  11.7 | max:   12\n","episode:  4/5 | treward:   11 | perf: 0.364 | av:  11.5 | max:   12\n","episode:  5/5 | treward:   12 | perf: 0.167 | av:  11.6 | max:   12\n","CPU times: user 88 ms, sys: 6.77 ms, total: 94.8 ms\n","Wall time: 213 ms\n"]}]},{"cell_type":"code","source":["torch.save(agent.model.state_dict(),'./data/Epi-5.pth')"],"metadata":{"id":"JLzMeo59JcSj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["########################### CHECK #################################"],"metadata":{"id":"6veRQ2-CK6ah"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import pickle\n","import torch\n","import torch.nn as nn\n","\n","with open('train_N.pickle','rb') as f:\n","    data = pickle.load(file = f)#data=> list\n","dataN = len(data)\n","\n","df = pd.DataFrame(data, columns =['Price', 'Symb'])\n","df.insert(2, \"dailyR\", np.nan)\n","for i in range(1,dataN):\n","  if df['Symb'][i][3:5] == df['Symb'][i-1][3:5]:\n","    df.at[i, 'dailyR'] = np.log(df['Price'][i].iloc[283,1])-np.log(df['Price'][i-1].iloc[283,1])\n","\n","clP = np.zeros(dataN)\n","for j in range(dataN):\n","  clP[j] = df['Price'][j].iloc[283,1]#每日13:29的價\n","pM = clP.mean()\n","pS = clP.std()\n","clPNorm =  (clP - pM) / pS\n","\n","class observation_space:\n","    def __init__(self, n):\n","        self.shape = (n,)\n","\n","class action_space:\n","    def __init__(self, n):\n","        self.n = n\n","    def seed(self, seed):\n","        pass\n","    def sample(self):\n","        return random.randint(0, self.n - 1)\n","\n","class Finance:\n","    def __init__(self, features, window, lags):\n","        self.features = features\n","        self.observation_space = observation_space(len(features))\n","        self.action_space = action_space(3)#ori:2  \n","        self.window = window#如5日均值\n","        self.lags = lags #相當於batach_size\n","        self.osn = self.lags       \n","        self.min_accuracy = 0.475\n","        self._prepare_data()\n","\n","    def _prepare_data(self):\n","        self.data = df\n","        self.data['r'] = df['dailyR']\n","        self.data.dropna(inplace=True)       \n","        self.data['d'] = np.where(self.data['r'] > 0, 1, 2)\n","        self.data['clPNorm'] = pd.DataFrame(clPNorm)\n","        self.data['rNorm'] = (self.data['r']-self.data['r'].mean())/self.data['r'].std()\n","        self.data['clPNormAvg'] = self.data['clPNorm'].rolling(self.window).mean()\n","        self.data['clPNormStd'] = self.data['clPNorm'].rolling(self.window).std()\n","        self.data['rNormAvg'] = self.data['rNorm'].rolling(self.window).mean()\n","        self.data['rNormStd'] = self.data['rNorm'].rolling(self.window).std()\n","        self.data.dropna(inplace=True)#這行會使平均值,標準差的NaN都消失\n","\n","    def _get_state(self):\n","        return self.data[self.features].iloc[self.bar-self.lags : self.bar]\n","\n","    def reset(self):\n","        self.treward = 0\n","        self.accuracy = 0\n","        self.bar = self.osn#必須先要reset才會有self.bar, 使用step(a)才不會出錯\n","        state = self.data[self.features].iloc[self.bar-self.osn : self.bar]#會按照順序\n","        return state.values\n","\n","    def step(self, action):\n","        correct = action == self.data['d'].iloc[self.bar]\n","        reward_1 = 1 if correct else 0\n","        self.treward += reward_1\n","        self.bar += 1\n","        self.accuracy = self.treward / (self.bar - self.osn)\n","\n","        if self.bar >= len(self.data):\n","            done = True\n","        elif reward_1 == 1:\n","            done = False\n","        elif ((self.accuracy < self.min_accuracy) and (self.bar > self.osn + 10)):# Xa.ipynb\n","            done = True\n","        else:\n","            done = False\n","      \n","        state = self._get_state()\n","        info = {}\n","\n","        return state.values, reward_1, done, info\n","\n","class CategoricalPolicy(nn.Module):\n","    def __init__(self, hidden_size, state_dim=6, action_dim=3):#\n","        super(CategoricalPolicy, self).__init__()\n","        self.state_dim = state_dim\n","        self.action_dim = action_dim\n","\n","        ######################## Q1 Head ##################\n","        # Construct Hidden Layer 1 with state\n","        self.f1 = nn.Linear(state_dim, hidden_size)\n","\n","        #Hidden Layer 2\n","        self.f2 = nn.Linear(hidden_size, hidden_size)\n","\n","        #Value\n","        self.val = nn.Linear(hidden_size, 1)\n","\n","        #Advantages\n","        self.adv = nn.Linear(hidden_size, action_dim)\n","\n","\n","\n","    def clean_action(self, obs, return_only_action=True):\n","        ###### Feature ####\n","        info = torch.relu(self.f1(obs))#RuntimeError: mat1 and mat2 must have the same dtype\n","        info = torch.relu(self.f2(info))\n","\n","        val = self.val(info)\n","        adv = self.adv(info)\n","\n","        logits = val + adv - adv.mean()\n","\n","        if return_only_action:\n","            return logits.argmax(1)\n","\n","        return None, None, logits\n","\n","    def noisy_action(self, obs, return_only_action=True):\n","        _, _, logits = self.clean_action(obs, return_only_action=False)\n","        dist = Categorical(logits=logits)\n","        action = dist.sample()\n","        action = action\n","\n","        if return_only_action:\n","            return action\n","\n","        return action, None, logits\n","\n","    def act(self, state):\n","        state = torch.tensor(state, dtype=torch.float32)\n","        \n","        #action, _, _ = self.model.noisy_action(state, return_only_action=False)\n","        action = self.clean_action(state, return_only_action=True)\n","        #print('aaaaaaaaaaaaaaa  ',action)\n","\n","        #return torch.argmax(action)\n","        return action[0]\n","\n","features = ['clPNorm','rNorm','clPNormAvg','clPNormStd','rNormAvg','rNormStd']\n","window = 5 #平均值時會用到\n","lags = 4\n","num_hidden = 256\n","test_env = Finance(features, window, lags)#\n","\n","policy_net = CategoricalPolicy(num_hidden)\n","#state_dict = torch.load('/content/Epi-42689.pth')\n","state_dict = torch.load('/content/data/Epi-49514.pth')\n","policy_net.load_state_dict(state_dict)\n","\n","test_episodes = 50\n","trewards = []\n","for e in range(test_episodes):\n","  state = test_env.reset()\n","  rewardT = 0\n","  for _ in range(10000):\n","    #print('sssssssssssss  ',state)\n","    action = policy_net.act(state)\n","    next_state, reward, done, info = test_env.step(action)\n","    #print('rrrrrrrrrrrrrrr  ',reward,'   ddddddddddddd  ', done)\n","    rewardT += reward\n","    state = next_state\n","\n","    if done:\n","      treward = _ + 1\n","      trewards.append(treward)\n","      av = sum(trewards[-len(trewards):]) / len(trewards)\n","      print('trewardtreward: ',treward,' rewardT: ',rewardT,' trewardAvg: ',av)\n","      \n","      break\n","\n","    "],"metadata":{"id":"NIVEFSQZMTEW"},"execution_count":null,"outputs":[]}]}