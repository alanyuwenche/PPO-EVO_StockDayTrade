{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"fQ5t0lpLdY_D","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1672196720457,"user_tz":-480,"elapsed":1031,"user":{"displayName":"Yu Alan","userId":"07954502632401534355"}},"outputId":"1678d98c-b361-476b-cd90-701dbb09003b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'PPO-EVO_StockDayTrade'...\n","remote: Enumerating objects: 13, done.\u001b[K\n","remote: Counting objects: 100% (13/13), done.\u001b[K\n","remote: Compressing objects: 100% (12/12), done.\u001b[K\n","remote: Total 13 (delta 2), reused 0 (delta 0), pack-reused 0\u001b[K\n","Unpacking objects: 100% (13/13), done.\n"]}],"source":["!git clone https://github.com/alanyuwenche/PPO-EVO_StockDayTrade"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":76},"executionInfo":{"elapsed":36065,"status":"ok","timestamp":1672196796441,"user":{"displayName":"Yu Alan","userId":"07954502632401534355"},"user_tz":-480},"id":"Z6uryOk9NpCK","outputId":"4bd59a9f-11d9-4d72-a93d-99ceada5f7d5"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-b14562fb-8ca9-4428-b73f-c69b791b24f2\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-b14562fb-8ca9-4428-b73f-c69b791b24f2\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving data-1225.zip to data-1225.zip\n"]}],"source":["from google.colab import files\n","uploaded = files.upload()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":293,"status":"ok","timestamp":1672196756599,"user":{"displayName":"Yu Alan","userId":"07954502632401534355"},"user_tz":-480},"id":"tUIxPFOEN0ma","outputId":"3b0abe94-f3e8-4dfe-e90a-30d3627d3e1b"},"outputs":[{"output_type":"stream","name":"stdout","text":["gym_wrapper.py  test_N2.pickle  test_N3.pickle  train_N.pickle\n"]}],"source":["ls"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":335,"status":"ok","timestamp":1672196749064,"user":{"displayName":"Yu Alan","userId":"07954502632401534355"},"user_tz":-480},"id":"sI_lT14w6S0T","outputId":"852c3eed-94a3-4c43-cb0d-3eb86ca3f933"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/PPO-EVO_StockDayTrade\n"]}],"source":["cd PPO-EVO_StockDayTrade/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XeFZ2gVN6XzG"},"outputs":[],"source":["rm -f E*.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eAF-Q5kK6dvJ"},"outputs":[],"source":["cd .."]},{"cell_type":"code","source":["!tar -cvf data.tar data/\n","#!gzip data.tar\n","#!tar zxvf data.tar.gz #解壓縮(須先改名)\n","#!unzip data.zip"],"metadata":{"id":"roEOBwDUwFA5"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8aTGujUHzewJ"},"outputs":[],"source":["!unzip data.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i2URKHpkneiu"},"outputs":[],"source":["!pip install tensorboardX"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1f7Ltu47nnqt"},"outputs":[],"source":["#%load_ext tensorboard\n","%reload_ext tensorboard\n","%tensorboard --logdir=/content/PPO-EVO_StockDayTrade/runs/20221220"]},{"cell_type":"code","source":["#### 預先載入ppo訓練模型ppoS-113400(已修改於erl_trainer_evo.py), 再以EVO訓練  ############################\n","#20230102- (test_N>4) and (no_T>1)=> 不理想\n","!python main.py --env daytradeEnv --popsize 20 --hidden_size 256 --num_test 10 --total_steps 120\n","#20230101- (test_N>4) and (no_T>1)=> 無結果\n","#!python main.py --env daytradeEnv --popsize 50 --hidden_size 256 --num_test 10 --total_steps 120\n","#20221230- (test_N>4) and (no_T>1)=> 不理想\n","#!python main.py --env daytradeEnv --popsize 30 --hidden_size 256 --num_test 10 --total_steps 120\n","#20221229- (test_N>2) and (no_T>0)=> Gen598或許可試\n","#!python main.py --env daytradeEnv --popsize 20 --hidden_size 256 --num_test 10 --total_steps 120"],"metadata":{"id":"RnCGIZCONS7-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zrJp7o71EJkR"},"source":["**@@@@@@@@@@@@@@@@@@@@@@@@@  PPO 訓練  @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uulVkr8uuUyF"},"outputs":[],"source":["#20221225 update_epoch = 16, 50天平均值=> 平均值大表現不會較好; ppoS-113400再用evo訓練(因交易週期短次數多)\n","#20221222 update_epoch = 16, 20天平均值=> 平均值大表現不會較好; 交易次數太多\n","import os\n","import torch\n","import torch.nn as nn\n","from torch.distributions import Categorical\n","import numpy as np\n","import pickle\n","import random\n","from gym_wrapper import *\n","\n","from collections import Counter\n","from tensorboardX import SummaryWriter\n","\n","# set device to cpu or cuda\n","device = torch.device('cpu')\n","\n","if(torch.cuda.is_available()): \n","    device = torch.device('cuda:0') \n","    torch.cuda.empty_cache()\n","    print(\"Device set to : \" + str(torch.cuda.get_device_name(device)))\n","else:\n","    print(\"Device set to : cpu\")\n","\n","class Reporter:\n","    def __init__(self, report_interval: int = 1):\n","        self.counter = Counter()\n","        self.graph_initialized = False\n","        self.report_interval = report_interval\n","        self.t = 0\n","        self.writer = SummaryWriter('runs/20221220')#20221004\n","\n","    def will_report(self, tag: str) -> bool:\n","        return self.counter[tag] % (self.report_interval + 1) == 0\n","\n","    def scalar(self, tag: str, value: float):\n","        if self.will_report(tag):\n","            self._scalar(tag, value, self.counter[tag])\n","        self.counter[tag] += 1\n","\n","    def graph(self, model, input_to_model):\n","        if not self.graph_initialized:\n","            self._graph(model, input_to_model)\n","            self.graph_initialized = True\n","\n","    def _scalar(self, tag: str, value: float, step: int):\n","        self.writer.add_scalar(tag, value, step)\n","\n","class RolloutBuffer:\n","    def __init__(self):\n","        self.actions = []\n","        self.states = []\n","        self.logprobs = []\n","        self.rewards = []\n","        self.is_terminals = []\n","    \n","\n","    def clear(self):\n","        del self.actions[:]\n","        del self.states[:]\n","        del self.logprobs[:]\n","        del self.rewards[:]\n","        del self.is_terminals[:]\n","\n","class CategoricalPolicy(nn.Module):\n","    def __init__(self, state_dim, action_dim, hidden_size):\n","        super(CategoricalPolicy, self).__init__()\n","        self.action_dim = action_dim\n","\n","        ######################## Q1 Head ##################\n","        # Construct Hidden Layer 1 with state\n","        self.f1 = nn.Linear(state_dim, hidden_size)\n","\n","        #Hidden Layer 2\n","        self.f2 = nn.Linear(hidden_size, hidden_size)\n","\n","        #Value\n","        self.val = nn.Linear(hidden_size, 1)\n","\n","        #Advantages\n","        self.adv = nn.Linear(hidden_size, action_dim)\n","\n","    def clean_action(self, obs, return_only_action=True):\n","        ###### Feature ####\n","        info = torch.relu(self.f1(obs))\n","        info = torch.relu(self.f2(info))\n","\n","        val = self.val(info)\n","        adv = self.adv(info)\n","\n","        logits = val + adv - adv.mean()\n","\n","        if return_only_action:\n","            return logits.argmax(1)\n","\n","        #return None, None, logits\n","        return val, adv, logits\n","\n","    def noisy_action(self, obs, return_only_action=True):\n","        _, _, logits = self.clean_action(obs, return_only_action=False)\n","\n","        dist = Categorical(logits=logits)\n","        action = dist.sample()\n","\n","        action = action\n","        action_logprob = dist.log_prob(action)\n","\n","        if return_only_action:\n","            return action\n","\n","        #return action, None, logits\n","        return action, action_logprob, logits\n","\n","\n","class PPO:\n","    def __init__(self, state_dim, action_dim, hidden_size, lr_actor, lr_critic, gamma, K_epochs, eps_clip):\n","        self.gamma = gamma\n","        self.eps_clip = eps_clip\n","        self.K_epochs = K_epochs\n","        self.buffer = RolloutBuffer()\n","        self.policy = CategoricalPolicy(state_dim, action_dim, hidden_size).to(device)\n","        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr_actor)\n","        \"\"\"\n","        self.actor = self.policy.adv#????????????????????????\n","        self.critic = self.policy.val\n","        self.optimizer = torch.optim.Adam([\n","                        {'params': self.actor.parameters(), 'lr': lr_actor},\n","                        {'params': self.critic.parameters(), 'lr': lr_critic}\n","                    ])\n","        \"\"\"\n","        self.policy_old = CategoricalPolicy(state_dim, action_dim, hidden_size).to(device)\n","        self.policy_old.load_state_dict(self.policy.state_dict())\n","        self.MseLoss = nn.MSELoss()\n","        self.reporter = Reporter()\n","\n","    def select_action(self, state):\n","\n","        with torch.no_grad():\n","            state = torch.FloatTensor(state).to(device)\n","            action, action_logp, _ = self.policy_old.noisy_action(state,return_only_action=False)\n","        \n","        self.buffer.states.append(state)\n","        self.buffer.actions.append(action)\n","        self.buffer.logprobs.append(action_logp)\n","\n","        return action.item()\n","\n","    def evaluate(self, state, action):\n","\n","        _, _, action_probs = self.policy.noisy_action(state, return_only_action=False)\n","        dist = Categorical(logits=action_probs)\n","        action_logprobs = dist.log_prob(action)\n","        dist_entropy = dist.entropy()\n","        #state_values = self.critic(state)\n","        state_values, _, _ = self.policy.clean_action(state, return_only_action=False)\n","        \n","        return action_logprobs, state_values, dist_entropy\n","\n","    def update(self):\n","        # Monte Carlo estimate of returns\n","        rewards = []\n","        discounted_reward = 0\n","        for reward, is_terminal in zip(reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)):\n","            if is_terminal:\n","                discounted_reward = 0\n","            discounted_reward = reward + (self.gamma * discounted_reward)\n","            rewards.insert(0, discounted_reward)\n","\n","        # Normalizing the rewards\n","        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n","        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n","\n","        # convert list to tensor\n","        old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach().to(device)\n","        old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach().to(device)\n","        old_logprobs = torch.squeeze(torch.stack(self.buffer.logprobs, dim=0)).detach().to(device)\n","        \n","        # Optimize policy for K epochs\n","        for _ in range(self.K_epochs):\n","\n","            # Evaluating old actions and values\n","            logprobs, state_values, dist_entropy = self.evaluate(old_states, old_actions)\n","\n","            # match state_values tensor dimensions with rewards tensor\n","            state_values = torch.squeeze(state_values)\n","            # Finding the ratio (pi_theta / pi_theta__old)\n","            #ratios = torch.exp(logprobs - old_logprobs.detach())#ori\n","            ratios = torch.exp(logprobs.detach() - old_logprobs)\n","\n","            # Finding Surrogate Loss\n","            advantages = rewards - state_values.detach()   \n","            surr1 = ratios * advantages\n","            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n","            policy_loss = -torch.min(surr1, surr2)\n","            value_loss = self.MseLoss(state_values, rewards)\n","            # final loss of clipped objective PPO\n","            #loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy\n","            loss = policy_loss + 0.5*value_loss - 0.01*dist_entropy\n","            loss = loss.mean()\n","\n","            # take gradient step\n","            self.optimizer.zero_grad()\n","            loss.backward()\n","            self.optimizer.step()\n","\n","        self.reporter.scalar('ppo_loss/total', loss)\n","        self.reporter.scalar('ppo_loss/policy_loss', policy_loss.mean().item())\n","        self.reporter.scalar('ppo_loss/value_loss', value_loss.mean().item())\n","\n","        # Copy new weights into old policy\n","        self.policy_old.load_state_dict(self.policy.state_dict())\n","\n","        # clear buffer\n","        self.buffer.clear()\n","    \n","print(\"============================================================================================\")\n","\n","def seed_torch(seed):\n","        torch.manual_seed(seed)\n","        if torch.backends.cudnn.enabled:\n","            torch.backends.cudnn.benchmark = False\n","            torch.backends.cudnn.deterministic = True\n","\n","def calDN(rew):\n","  rewL = len(rew)\n","  noT = 0\n","  for i in range(rewL):\n","    if abs(rew[i]) < 5:\n","      noT += 1\n","  return noT, rewL\n","\n","\n","\n","max_ep_len = 300       # max timesteps in one episode\n","num_epochs = 5000000    \n","update_epoch = 16      # update policy every n epoch\n","\n","\n","# Set the seed\n","seed = 111\n","np.random.seed(seed)\n","random.seed(seed)\n","seed_torch(seed)\n","\n","################ PPO hyperparameters ################\n","hidden_dim = 256\n","K_epochs = 40               # update policy for K epochs\n","eps_clip = 0.2              # clip parameter for PPO\n","gamma = 0.99                # discount factor\n","\n","lr_actor = 0.0003       # learning rate for actor network\n","lr_critic = 0.001       # learning rate for critic network\n","\n","#####################################################\n","\n","env_name = \"daytradeEnv_PPO\"\n","print(\"training environment name : \" + env_name)\n","env = GymWrapper(env_name)\n","num_inputs = env.state_dim\n","num_actions = env.action_dim\n","\n","############# print all hyperparameters #############\n","\n","print(\"--------------------------------------------------------------------------------------------\")\n","print(\"max timesteps per episode : \", max_ep_len)\n","print(\"PPO K epochs : \", K_epochs)\n","print(\"PPO epsilon clip : \", eps_clip)\n","print(\"discount factor (gamma) : \", gamma)\n","\n","print(\"--------------------------------------------------------------------------------------------\")\n","\n","print(\"optimizer learning rate actor : \", lr_actor)\n","print(\"optimizer learning rate critic : \", lr_critic)\n","\n","################# training procedure ################\n","\n","# initialize a PPO agent\n","ppo_agent = PPO(num_inputs, num_actions, hidden_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip)\n","\n","print(\"============================================================================================\")\n","\n","os.makedirs('./data/', exist_ok=True)\n","ret = []\n","for i_episode in range(num_epochs):    \n","    state = env.reset()\n","    current_ep_reward = 0\n","\n","    for t in range(max_ep_len):\n","        \n","        # select action with policy\n","        action = ppo_agent.select_action(state)\n","        state_next, reward, done, _ = env.step(action)\n","        \n","        # saving reward and is_terminals\n","        ppo_agent.buffer.rewards.append(reward)\n","        ppo_agent.buffer.is_terminals.append(done)\n","        current_ep_reward += reward\n","\n","        state = state_next\n","        if done:\n","            ret.append(current_ep_reward)\n","            ppo_agent.reporter.scalar('Reward/single_R', current_ep_reward)\n","            if i_episode > 5 and i_episode % update_epoch == 0:\n","            #if i_episode % update_epoch == 0:\n","                ppo_agent.update()\n","            break\n","\n","    if i_episode > 5 and i_episode % 50 == 0:\n","      noTrade, totTrade = calDN(ret)     \n","      avgR = np.array(ret).mean()\n","      ppo_agent.reporter.scalar('Reward/avg_R', avgR)\n","      if avgR > 200:\n","        f = open(\"./data/logfile.txt\",\"a\")\n","        f.write(\"i_episode: %d\\t\" % i_episode)\n","        f.write(\"avg_reward: %d\\t\" % avgR)\n","        f.write(\"交易次數: %d\\t\" % (totTrade-noTrade))\n","        f.write(\"總天數: %d\\t\" % totTrade)\n","        f.write(\"\\n\")\n","        f.close()\n","        fileN = './data/ppoS-'+str(i_episode)+'.pth'\n","        torch.save(ppo_agent.policy.state_dict(),fileN)\n","      print(\"i_episode: \", i_episode, \" avg_reward: \",avgR,\" 交易次數: \",(totTrade-noTrade),\"/\",totTrade)\n","      #print(\"i_episode: \", i_episode, \" avg_reward: \",avgR,\" 交易次數: \",noTrade,\"/\",totTrade)\n","      ppo_agent.reporter.scalar('ppo_loss/noTrade', noTrade)\n","      del ret[:] \n","\n"]},{"cell_type":"code","source":["policy_net._modules['adv'].weight.detach().numpy().shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lHcQWSmJx1eg","executionInfo":{"status":"ok","timestamp":1671544053783,"user_tz":-480,"elapsed":287,"user":{"displayName":"Yu Alan","userId":"07954502632401534355"}},"outputId":"d9d5ba6f-bf6c-4f8f-ca3b-6644a5c2a946"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3, 256)"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"0E14lj10Y6vZ"},"source":["**###########  測試結果  ##############################**"]},{"cell_type":"code","source":["#測試模型- train_N, test_N, test_N2\n","import pickle\n","import numpy as np\n","import random\n","import gym\n","\n","def standardScale_S(aa):\n","    K = 3.29 #99.9%\n","    #K = 1.96 #95%\n","    meanV = 0.00088012709079\n","    stdV = 0.0104573489\n","    x_std = (aa-meanV)/(K*stdV)\n","    return x_std, meanV, stdV \n","\n","def standardScale_P(aa):\n","    K = 3.29 #99.9%\n","    #K = 1.96 #95%\n","    meanV = 11136.855753\n","    stdV = 1094.2235127\n","    x_std = (aa-meanV)/(K*stdV)\n","    return x_std, meanV, stdV   \n","\n","with open('/content/PPO-EVO_StockDayTrade/train_N.pickle','rb') as f:\n","#with open('test_N2.pickle','rb') as f:\n","    data = pickle.load(file = f)\n","l = 284 #13:29=> the last one\n","dataN = len(data)\n","ss = np.zeros((l,dataN))\n","pp = np.zeros((l,dataN))\n","symBe = ''\n","base = 0\n","\n","for i in range(dataN):\n","    sym = (data[i][1])[3:4] #J, K, L\n","    if sym != symBe:\n","\n","        #for s in range(l-1):\n","        for s in range(l):\n","            #ss[s,i] = (data[i][0].iloc[s,1]/data[i][0].iloc[0,1]-1.)\n","            ss[s,i] = np.log(data[i][0].iloc[s,1])-np.log(data[i][0].iloc[0,1])\n","            pp[s,i] = data[i][0].iloc[s,1]\n","        symBe = sym\n","        base = data[i][0].iloc[l-1,1]\n","\n","    else:\n","        #for s in range(l-1):\n","        for s in range(l):\n","            #ss[s,i] = (data[i][0].iloc[s,1]/base-1.)\n","            ss[s,i] = np.log(data[i][0].iloc[s,1])-np.log(base)\n","            pp[s,i] = data[i][0].iloc[s,1]\n","        symBe = sym\n","        base = data[i][0].iloc[l-1,1]            \n","\n","ss, _, _ = standardScale_S(ss)#如果只是測試training data沒變, 但testing data會有差\n","pp, _, _ = standardScale_P(pp)\n","\n","class GymWrapper(gym.Env):\n","\n","  def __init__(self, env_name, frameskip=None):\n","      self.state_dim = 6\n","      self.action_dim = 3\n","      self.pIndex = ss\n","      self.price_std = pp\n","      self.price = data\n","      self.commission = 47\n","      self.position = np.array([0.])\n","      self.inventory = []\n","      self.d = 0 # day\n","      self.t = 0 # time\n","      self.st = 0 # tradin time\n","      self.done = False\n","\n","  def is_discrete(self, env):\n","        try:\n","            k = env.action_space.n\n","            return True\n","        except:\n","            return False\n","\n","  def getStateTv(self):\n","      aa = self.pIndex[self.t-4:self.t , self.d]\n","      pri_s = np.array([self.price_std[self.t-1, self.d]])\n","      aa = np.concatenate((aa, pri_s, self.position), axis=0)\n","      return aa\n","\n","  def _take_action(self, action):\n","      reward = 0\n","      if action == 1:\n","          if int(self.position[0]) == 0:\n","              self.position = np.array([1.])\n","              self.inventory.append(self.price[self.d][0].iloc[self.t-1,1])\n","              self.st = self.t # tradin time\n","\n","          if int(self.position[0]) == -1:\n","              sold_price = self.inventory.pop(0)\n","              reward = 50*(sold_price - self.price[self.d][0].iloc[self.t-1,1])-2*self.commission\n","              self.done = True\n","              self.position = np.array([0.])\n","      elif action == 2:\n","          if int(self.position[0]) == 0:\n","              self.position = np.array([-1.])\n","              self.inventory.append(self.price[self.d][0].iloc[self.t-1,1])\n","              self.st = self.t # tradin time\n","\n","          if int(self.position[0]) == 1:\n","              bought_price = self.inventory.pop(0)\n","              reward = 50*(self.price[self.d][0].iloc[self.t-1,1] - bought_price)-2*self.commission\n","              self.done = True\n","              self.position = np.array([0.])\n","       \n","      return reward, self.position\n","\n","  def step(self, action):\n","      reward, self.position = self._take_action(action)\n","      self.t += 1\n","      #observation = self.getState()\n","      observation = self.getStateTv()\n","      if self.t == 284:\n","          self.done = True\n","          if len(self.inventory) > 0:\n","              if int(self.position[0]) == 1:\n","                  bought_price = self.inventory.pop(0)\n","                  reward = 50*(self.price[self.d][0].iloc[self.t-1,1] - bought_price)-2*self.commission\n","                  #observation[self.state_dim+1] = np.array([0.])\n","                  observation[5] = np.array([0.])#20220509\n","\n","              elif int(self.position[0]) == -1:\n","                  sold_price = self.inventory.pop(0)\n","                  reward = 50*(sold_price - self.price[self.d][0].iloc[self.t-1,1])-2*self.commission\n","                  #observation[self.state_dim+1] = np.array([0.])\n","                  observation[5] = np.array([0.])#20220509\n","      info = {'env.d':self.d, 'env.t':self.t, 'reward':reward, 'tradein t':self.st}\n","      #print('observation  ',observation)\n","      #print('self.done  ',self.done)\n","      return observation, reward, self.done, info\n","\n","  def reset(self):\n","      self.position = np.array([0.])\n","      self.inventory = []\n","      dth = random.randint(0, self.pIndex.shape[1]-1)\n","      self.d = dth # day\n","      self.t = 4 # time\n","      self.st = 4 # tradin time\n","      self.done = False\n","\n","      return self.getStateTv() "],"metadata":{"id":"euwPn3UMv5PG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class CategoricalPolicy(nn.Module):\n","    def __init__(self, state_dim, action_dim, hidden_size):\n","        super(CategoricalPolicy, self).__init__()\n","        self.state_dim = state_dim\n","        self.action_dim = action_dim\n","        self.f1 = nn.Linear(state_dim, hidden_size)\n","        self.f2 = nn.Linear(hidden_size, hidden_size)\n","        self.val = nn.Linear(hidden_size, 1)\n","        #Advantages\n","        self.adv = nn.Linear(hidden_size, action_dim)\n","\n","    def forward(self, x):\n","        x = torch.from_numpy(x).float().unsqueeze(0)\n","        x = F.relu(self.f1(x))\n","        x = F.relu(self.f2(x))\n","        val = self.val(x)\n","        adv = self.adv(x)\n","        logits = val + adv - adv.mean()\n","\n","        return logits.argmax(1).item()\n","\n","env = GymWrapper(\"daytradeEnv\")\n","num_inputs = env.state_dim\n","num_actions = env.action_dim \n","num_hidden = 256\n","\n","policy_net = CategoricalPolicy(num_inputs, num_actions, num_hidden)\n","state_dict = torch.load('/content/PPO-EVO_StockDayTrade/data/ppoS-113400.pth')\n","#state_dict = torch.load('/content/Evolutionary-Reinforcement-Learning/data/Gen-2542.pth',map_location=torch.device('cpu')) # GPU用\n","policy_net.load_state_dict(state_dict)\n","\"\"\"\n","np.savetxt(\"f1_weight.csv\",policy_net._modules['f1'].weight.detach().numpy() ,delimiter=\",\",fmt='%.6f')\n","np.savetxt(\"f1_bias.csv\",policy_net._modules['f1'].bias.detach().numpy() ,delimiter=\",\",fmt='%.6f')\n","np.savetxt(\"f2_weight.csv\",policy_net._modules['f2'].weight.detach().numpy() ,delimiter=\",\",fmt='%.6f')\n","np.savetxt(\"f2_bias.csv\",policy_net._modules['f2'].bias.detach().numpy() ,delimiter=\",\",fmt='%.6f')\n","np.savetxt(\"val_weight.csv\",policy_net._modules['val'].weight.detach().numpy() ,delimiter=\",\",fmt='%.6f')\n","np.savetxt(\"val_bias.csv\",policy_net._modules['val'].bias.detach().numpy() ,delimiter=\",\",fmt='%.6f')\n","np.savetxt(\"adv_weight.csv\",policy_net._modules['adv'].weight.detach().numpy() ,delimiter=\",\",fmt='%.6f')\n","np.savetxt(\"adv_bias.csv\",policy_net._modules['adv'].bias.detach().numpy() ,delimiter=\",\",fmt='%.6f')\n","\"\"\"\n","test_epochs = 50\n","rewards = []\n","for i_episode in range(test_epochs):\n","    state = env.reset()\n","    current_ep_reward = 0\n","    #env.d = 6 # 同時變更 同時變更 同時變更 同時變更\n","    #state = env.getStateTv() # 同時變更 同時變更 同時變更 同時變更\n","    for t in range(300):\n","        #print('sssssssssss state:',state)\n","        action = policy_net(state)\n","        #print('env.d: ',env.d,' env.t: ',env.t,' action: ',action)#!!!!實際驗證盈虧時是用t-1的價格\n","        state, reward, done, _ = env.step(action)\n"," \n","        current_ep_reward += reward\n","\n","        if done:\n","            print('i_episode: ',i_episode,' current_ep_reward: ',current_ep_reward,' env.d: ',env.d)\n","            rewards.append(current_ep_reward)\n","            break\n","\n","avg_reward = np.array(rewards)\n","x = np.where(avg_reward == 0)\n","avg_R = np.delete(avg_reward, x)\n","#print('Average reward: {:.2f}'.format(np.mean(avg_R)))\n","print('Average reward: ', np.mean(avg_R),' numTrade: ',avg_R.shape[0])"],"metadata":{"id":"08BKQbgSxJ8W"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}